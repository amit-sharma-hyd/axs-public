{
 "metadata": {
  "name": "",
  "signature": "sha256:b34756715def3f67c15678ee21ff6c20bfad5cfc4c5abdacdf8a19e48498fe91"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import decomposition, pipeline, metrics, grid_search\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "pd.set_option('display.mpl_style', 'default') \n",
      "pd.set_option('display.width', 5000) \n",
      "pd.set_option('display.max_columns', 60) \n",
      "\n",
      "train = pd.read_csv('C:\\\\axs\\\\work\\\\kaggle\\\\crowdflower\\\\input\\\\train.csv')\n",
      "\n",
      "# create labels. drop useless columns\n",
      "y = train.median_relevance.values\n",
      "train = train.drop(['median_relevance', 'relevance_variance'], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = train['query'].value_counts()\n",
      "len(counts) #261 unique queries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "261"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "# Plot the top 10 queries - not really required now but wanted to check how plots work ;)\n",
      "data = counts[:10]\n",
      "plt.rc('xtick', labelsize=8) \n",
      "plt.rc('ytick', labelsize=8) \n",
      "xpos = np.arange(len(data))\n",
      "plt.bar(xpos, data, align='center', alpha=0.4)\n",
      "plt.xlabel('Query')\n",
      "plt.ylabel('Frequency')\n",
      "plt.title('Query Hist')\n",
      "plt.xticks(xpos, data.index)\n",
      "plt.show()\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "\"\\n# Plot the top 10 queries - not really required now but wanted to check how plots work ;)\\ndata = counts[:10]\\nplt.rc('xtick', labelsize=8) \\nplt.rc('ytick', labelsize=8) \\nxpos = np.arange(len(data))\\nplt.bar(xpos, data, align='center', alpha=0.4)\\nplt.xlabel('Query')\\nplt.ylabel('Frequency')\\nplt.title('Query Hist')\\nplt.xticks(xpos, data.index)\\nplt.show()\\n\""
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The following 3 functions have been taken from Ben Hamner's github repository\n",
      "# https://github.com/benhamner/Metrics\n",
      "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
      "    \"\"\"\n",
      "    Returns the confusion matrix between rater's ratings\n",
      "    \"\"\"\n",
      "    assert(len(rater_a) == len(rater_b))\n",
      "    if min_rating is None:\n",
      "        min_rating = min(rater_a + rater_b)\n",
      "    if max_rating is None:\n",
      "        max_rating = max(rater_a + rater_b)\n",
      "    num_ratings = int(max_rating - min_rating + 1)\n",
      "    conf_mat = [[0 for i in range(num_ratings)]\n",
      "                for j in range(num_ratings)]\n",
      "    for a, b in zip(rater_a, rater_b):\n",
      "        conf_mat[a - min_rating][b - min_rating] += 1\n",
      "    return conf_mat\n",
      "\n",
      "\n",
      "def histogram(ratings, min_rating=None, max_rating=None):\n",
      "    \"\"\"\n",
      "    Returns the counts of each type of rating that a rater made\n",
      "    \"\"\"\n",
      "    if min_rating is None:\n",
      "        min_rating = min(ratings)\n",
      "    if max_rating is None:\n",
      "        max_rating = max(ratings)\n",
      "    num_ratings = int(max_rating - min_rating + 1)\n",
      "    hist_ratings = [0 for x in range(num_ratings)]\n",
      "    for r in ratings:\n",
      "        hist_ratings[r - min_rating] += 1\n",
      "    return hist_ratings\n",
      "\n",
      "\n",
      "def quadratic_weighted_kappa(y, y_pred):\n",
      "    \"\"\"\n",
      "    Calculates the quadratic weighted kappa\n",
      "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
      "    value, which is a measure of inter-rater agreement between two raters\n",
      "    that provide discrete numeric ratings.  Potential values range from -1\n",
      "    (representing complete disagreement) to 1 (representing complete\n",
      "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
      "    chance.\n",
      "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
      "    each correspond to a list of integer ratings.  These lists must have the\n",
      "    same length.\n",
      "    The ratings should be integers, and it is assumed that they contain\n",
      "    the complete range of possible ratings.\n",
      "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
      "    is the minimum possible rating, and max_rating is the maximum possible\n",
      "    rating\n",
      "    \"\"\"\n",
      "    rater_a = y\n",
      "    rater_b = y_pred\n",
      "    min_rating=None\n",
      "    max_rating=None\n",
      "    rater_a = np.array(rater_a, dtype=int)\n",
      "    rater_b = np.array(rater_b, dtype=int)\n",
      "    assert(len(rater_a) == len(rater_b))\n",
      "    if min_rating is None:\n",
      "        min_rating = min(min(rater_a), min(rater_b))\n",
      "    if max_rating is None:\n",
      "        max_rating = max(max(rater_a), max(rater_b))\n",
      "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
      "                                min_rating, max_rating)\n",
      "    num_ratings = len(conf_mat)\n",
      "    num_scored_items = float(len(rater_a))\n",
      "\n",
      "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
      "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
      "\n",
      "    numerator = 0.0\n",
      "    denominator = 0.0\n",
      "\n",
      "    for i in range(num_ratings):\n",
      "        for j in range(num_ratings):\n",
      "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
      "                              / num_scored_items)\n",
      "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
      "            numerator += d * conf_mat[i][j] / num_scored_items\n",
      "            denominator += d * expected_count / num_scored_items\n",
      "\n",
      "    return (1.0 - numerator / denominator)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train['query'] = train['query'].astype('str')\n",
      "train['product_title'] = train['product_title'].astype('str')\n",
      "\n",
      "# Lowercase everything\n",
      "train['query'] = train['query'].str.lower()\n",
      "train['product_title'] = train['product_title'].str.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def intersect(row):\n",
      "    return (set(row['query'].split()) & set(row['product_title'].split()))\n",
      "\n",
      "train['query_title'] = train.apply(intersect,axis=1)\n",
      "train['query_title_nwords'] = train['query_title'].str.len()\n",
      "train['title_nwords'] = train['product_title'].str.split().str.len()\n",
      "train['query_nwords'] = train['query'].str.split().str.len()\n",
      "train['query_title_match'] = train['query_title_nwords']/train['query_nwords']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X=train[['query', 'query_nwords','title_nwords','query_title_nwords','query_title_match']]\n",
      "X['query_len'] = train['query'].str.len()\n",
      "X['ptitle_len'] = train['product_title'].str.len()\n",
      "X['query_title_match'] = (X['query_title_nwords']+0.00001)/X['query_nwords']\n",
      "X['ptitle_size_over_query'] = X['ptitle_len']/X['query_len']\n",
      "X['query'] = X['query'].astype(str)\n",
      "\n",
      "# Create dummy columns for categorical variable as scikit does not understand factors\n",
      "X = pd.concat([X,pd.get_dummies(X['query'])],axis=1)\n",
      "\n",
      "X = X.drop(['query'], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "def multiclass_split(train, target, classes, testSize):\n",
      "    retTrainX = retTestX = retTrainY = retTestY  = []\n",
      "    for i in classes:\n",
      "        trainClass = train[target==i]\n",
      "        trainX, testX, trainY, testY = train_test_split(trainClass, np.repeat(i, len(trainClass)), test_size=testSize, random_state=42)\n",
      "        if (i==classes[0]):\n",
      "            retTrainX, retTestX, retTrainY, retTestY  =  trainX, testX, trainY, testY\n",
      "        else:\n",
      "            retTrainX = np.vstack([retTrainX,trainX])\n",
      "            retTestX = np.vstack([retTestX, testX])\n",
      "            retTrainY = np.concatenate([retTrainY, trainY])\n",
      "            retTestY = np.concatenate([retTestY, testY])\n",
      "    return (retTrainX, retTestX, retTrainY, retTestY)\n",
      "\n",
      "trainX, testX, trainY, testY = multiclass_split(X, y, range(1,5), 0.33)\n",
      "\n",
      "trainX = pd.DataFrame.from_dict(trainX) \n",
      "testX = pd.DataFrame.from_dict(testX)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "rf = RandomForestClassifier(n_estimators=100)\n",
      "rf.fit(trainX, trainY)\n",
      "pred = rf.predict(testX)\n",
      "print quadratic_weighted_kappa(testY, pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.525302022057\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "gbF = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5,\n",
      "                                 max_depth=1, random_state=0).fit(trainX, trainY)\n",
      "pred = gbF.predict(testX)\n",
      "print quadratic_weighted_kappa(testY, pred)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.450829097164\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "svc = OneVsRestClassifier(SVC(kernel='linear'))\n",
      "svc.fit(trainX[[1,2,3,4,5,6,7]], trainY)\n",
      "pred = svc.predict(testX[[1,2,3,4,5,6,7]])\n",
      "print quadratic_weighted_kappa(testY, pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}