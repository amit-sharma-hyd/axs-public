{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import decomposition, pipeline, metrics, grid_search\n\npd.set_option('display.width', 5000) \npd.set_option('display.max_columns', 60) \n\ntrain = pd.read_csv('C:\\\\axs\\\\work\\\\kaggle\\\\crowdflower\\\\input\\\\train.csv')\n\n# create labels. drop useless columns\ny = train.median_relevance.values\ntrain = train.drop(['median_relevance', 'relevance_variance'], axis=1)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 2, "cell_type": "code", "source": "# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 3, "cell_type": "code", "source": "def getTfIdfBasedPreds(train, y, test):\n    # do some lambda magic on text columns\n    traindata = list(train.apply(lambda x:'%s %s' % (x['query'],x['product_title']),axis=1))\n    testdata = list(test.apply(lambda x:'%s %s' % (x['query'],x['product_title']),axis=1))\n\n\n    # the infamous tfidf vectorizer (Do you remember this one?)\n    tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 5), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n    # Fit TFIDF\n    tfv.fit(traindata)\n    X =  tfv.transform(traindata) \n    X_test = tfv.transform(testdata)\n\n    # Initialize SVD\n    svd = TruncatedSVD()\n\n    # Initialize the standard scaler \n    scl = StandardScaler()\n\n    # We will use SVM here..\n    svm_model = SVC()\n\n    # Create the pipeline\n    clf = pipeline.Pipeline([('svd', svd),\n                             ('scl', scl),\n                             ('svm', svm_model)])\n\n    # Create a parameter grid to search for best parameters for everything in the pipeline\n    param_grid = {'svd__n_components' : [400],\n                  'svm__C': [12]}\n    \n    # Model Code - Run the model\n\n    # Kappa Scorer \n    kappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better = True)\n\n    # Initialize Grid Search Model\n    model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                     verbose=10, n_jobs=1, iid=True, refit=True, cv=2)\n\n    # Fit Grid Search Model\n    model.fit(X, trainY)\n    print(\"Best score: %0.3f\" % model.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = model.best_estimator_.get_params()\n    for param_name in sorted(param_grid.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n    # Get best model\n    best_model = model.best_estimator_\n    \n    trainPred = best_model.predict(X)\n    testPred = best_model.predict(X_test)\n   \n    return(trainPred, testPred)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 4, "cell_type": "code", "source": "train['query'] = train['query'].astype('str')\ntrain['product_title'] = train['product_title'].astype('str')\ntrain['product_description'] = train['product_description'].astype('str')\n\n# Lowercase everything\ntrain['query'] = train['query'].str.lower()\ntrain['product_title'] = train['product_title'].str.lower()\ntrain['product_description'] = train['product_description'].str.lower()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 5, "cell_type": "code", "source": "# Clean up the description\nfrom bs4 import BeautifulSoup\ntrain['pdesc'] = map(lambda x: BeautifulSoup(''.join(x)).text, train['product_description'])", "outputs": [{"output_type": "stream", "name": "stderr", "text": "C:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65497012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\nC:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65516012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\nC:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/6552101\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 6, "cell_type": "code", "source": "train[:3]\n", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "   id                      query                                      product_title                                product_description                                              pdesc\n0   1  bridal shower decorations        accent pillow with heart design - red/black  red satin accent pillow embroidered with a hea...  red satin accent pillow embroidered with a hea...\n1   2       led christmas lights  set of 10 battery operated multi led train chr...  set of 10 battery operated train christmas lig...  set of 10 battery operated train christmas lig...\n2   4                  projector         viewsonic pro8200 dlp multimedia projector                                                nan                                                nan", "text/html": "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>query</th>\n      <th>product_title</th>\n      <th>product_description</th>\n      <th>pdesc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td> 1</td>\n      <td> bridal shower decorations</td>\n      <td>       accent pillow with heart design - red/black</td>\n      <td> red satin accent pillow embroidered with a hea...</td>\n      <td> red satin accent pillow embroidered with a hea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td> 2</td>\n      <td>      led christmas lights</td>\n      <td> set of 10 battery operated multi led train chr...</td>\n      <td> set of 10 battery operated train christmas lig...</td>\n      <td> set of 10 battery operated train christmas lig...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td> 4</td>\n      <td>                 projector</td>\n      <td>        viewsonic pro8200 dlp multimedia projector</td>\n      <td>                                               nan</td>\n      <td>                                               nan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 7, "cell_type": "code", "source": "def query_title_common(row):\n    return (set(row['query'].split()) & set(row['product_title'].split()))\n\ndef query_desc_common(row):\n    return (set(row['query'].split()) & set(row['pdesc'].split()))\n\n\ntrain['query_title'] = train.apply(query_title_common,axis=1)\ntrain['query_title_nwords'] = train['query_title'].str.len()\ntrain['title_nwords'] = train['product_title'].str.split().str.len()\ntrain['query_nwords'] = train['query'].str.split().str.len()\ntrain['query_title_match'] = train['query_title_nwords']/train['query_nwords']\n\ntrain['query_desc'] = train.apply(query_desc_common,axis=1)\ntrain['query_desc_nwords'] = train['query_desc'].str.len()\ntrain['desc_nwords'] = train['pdesc'].str.split().str.len()\ntrain['query_nwords'] = train['query'].str.split().str.len()\ntrain['query_desc_match'] = train['query_desc_nwords']/train['query_nwords']", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 8, "cell_type": "code", "source": "X=pd.DataFrame(train[['query', 'product_title', 'pdesc', 'query_nwords','title_nwords',\n         'query_title_nwords','query_title_match', 'desc_nwords','query_desc_nwords','query_desc_match']])\nX['query_len'] = train['query'].str.len()\nX['ptitle_len'] = train['product_title'].str.len()\nX['query_title_match'] = (X['query_title_nwords']+0.00001)/X['query_nwords']\nX['ptitle_size_over_query'] = X['ptitle_len']/X['query_len']\nX['query'] = X['query'].astype(str)\n\n\nX['query_len'] = train['query'].str.len()\nX['pdesc_len'] = train['pdesc'].str.len()\nX['query_desc_match'] = (X['query_desc_nwords']+0.00001)/X['query_nwords']\nX['pdesc_size_over_query'] = X['pdesc_len']/X['query_len']", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "# Create dummy columns for categorical variable as scikit does not understand factors\n# X = pd.concat([X,pd.get_dummies(X['query'])],axis=1)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 10, "cell_type": "code", "source": "# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(y, n_iter=3, test_size=0.2)\n\nfor train_index, test_index in sss:\n    trainX, testX = X.iloc[train_index], X.iloc[test_index]\n    trainY, testY = y[train_index], y[test_index]\n    \n#Just check if the stratification works fine\nimport collections\ncollections.Counter(trainY), collections.Counter(testY)", "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "(Counter({4: 4937, 3: 1390, 2: 1181, 1: 619}),\n Counter({4: 1234, 3: 347, 2: 295, 1: 155}))"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 11, "cell_type": "code", "source": "#Use SVM based learning \ntrainX['model1_Pred'], testX['model1_Pred'] = getTfIdfBasedPreds(trainX, trainY, testX)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n[CV] svm__C=12, svd__n_components=400 ................................\n[CV] ....... svm__C=12, svd__n_components=400, score=0.544526 -  16.2s\n[CV] svm__C=12, svd__n_components=400 ................................\n[CV] ....... svm__C=12, svd__n_components=400, score=0.525988 -  15.7s"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:   16.2s\n[Parallel(n_jobs=1)]: Done   2 jobs       | elapsed:   32.0s\n"}, {"output_type": "stream", "name": "stdout", "text": "\nBest score: 0.535"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   32.0s finished\nC:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from IPython.kernel.zmq import kernelapp as app\n"}, {"output_type": "stream", "name": "stdout", "text": "\nBest parameters set:\n\tsvd__n_components: 400\n\tsvm__C: 12\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 12, "cell_type": "code", "source": "trainX = trainX.drop(['query', 'product_title', 'pdesc'], axis=1)\ntestX = testX.drop(['query', 'product_title', 'pdesc'], axis=1)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 13, "cell_type": "code", "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import cross_validation\nclf = RandomForestClassifier(n_estimators=50, max_depth=None, oob_score=True, \n                             min_samples_split=1, random_state=0).fit(trainX, trainY)\nkappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better = True)\nscores = cross_validation.cross_val_score(clf, trainX, trainY, scoring=kappa_scorer, cv=10)\nscores", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "array([ 0.88707442,  0.87787704,  0.88094169,  0.90214393,  0.90321065,\n        0.9144012 ,  0.90721577,  0.91002949,  0.92783448,  0.92380684])"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 14, "cell_type": "code", "source": "preds = clf.predict(testX)\nmetrics.accuracy_score(testY, preds)", "outputs": [{"execution_count": 14, "output_type": "execute_result", "data": {"text/plain": "0.65829640571147219"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 15, "cell_type": "code", "source": "from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=1, random_state=0).fit(trainX, trainY)\nscores = cross_validation.cross_val_score(clf, trainX, trainY, scoring=kappa_scorer, cv=10)\nscores", "outputs": [{"execution_count": 15, "output_type": "execute_result", "data": {"text/plain": "array([ 0.89841613,  0.88321624,  0.90322426,  0.91574109,  0.91813897,\n        0.93342302,  0.92390526,  0.92079594,  0.93514931,  0.93196521])"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 16, "cell_type": "code", "source": "preds = clf.predict(testX)\nmetrics.accuracy_score(testY, preds)", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "0.65977351058591827"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.8", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}