> library(infotheo)
> dp <- discretize(trainAug$duration_previous, disc="equalwidth", nbins=15)
> trainAug$duration_prev_disc <- dp$X
> head(dp)
   X
1  3
2 14
3  5
4  4
5  3
6 10
> totalRows <- nrow(trainAug)
> trainCustIds <- sample(unique(trainAug$customer_ID),size=0.7*totalRows,replace=F)
> trainRecs <- trainAug[which(trainAug$customer_ID %in% trainCustIds),]
> testRecs <- trainAug[which(!(trainAug$customer_ID %in% trainCustIds)),]
> numTrees <- 500
> outVar <- "G"
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value+risk_factor+duration_prev_disc",
+                                                    "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                    "costBin+A1+B1+C1+D1+E1+F1+G1+A2+B2+C2+D2+E2+F2+G2",
+                                                    "costDir+costDir1"),collapse="+"))),
+ #                "costDir+costDir1"),collapse="+"))),
+ #            cv.folds = 3,
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0502
     2        1.3590             nan     0.0100    0.0481
     3        1.3328             nan     0.0100    0.0461
     4        1.3076             nan     0.0100    0.0443
     5        1.2834             nan     0.0100    0.0426
     6        1.2602             nan     0.0100    0.0410
     7        1.2378             nan     0.0100    0.0396
     8        1.2163             nan     0.0100    0.0381
     9        1.1955             nan     0.0100    0.0367
    10        1.1754             nan     0.0100    0.0354
    20        1.0076             nan     0.0100    0.0254
    40        0.7923             nan     0.0100    0.0142
    60        0.6678             nan     0.0100    0.0083
    80        0.5931             nan     0.0100    0.0048
   100        0.5477             nan     0.0100    0.0030
   120        0.5190             nan     0.0100    0.0018
   140        0.5008             nan     0.0100    0.0011
   160        0.4888             nan     0.0100    0.0007
   180        0.4805             nan     0.0100    0.0005
   200        0.4747             nan     0.0100    0.0004
   220        0.4701             nan     0.0100    0.0003
   240        0.4665             nan     0.0100    0.0002
   260        0.4637             nan     0.0100    0.0001
   280        0.4614             nan     0.0100    0.0001
   300        0.4594             nan     0.0100    0.0001
   320        0.4577             nan     0.0100    0.0000
   340        0.4562             nan     0.0100    0.0001
   360        0.4549             nan     0.0100    0.0000
   380        0.4537             nan     0.0100    0.0000
   400        0.4526             nan     0.0100    0.0000
   420        0.4517             nan     0.0100   -0.0000
   440        0.4508             nan     0.0100    0.0000
   460        0.4499             nan     0.0100   -0.0000
   480        0.4491             nan     0.0100   -0.0000
   500        0.4484             nan     0.0100   -0.0000

> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)

FALSE  TRUE 
 3809 25294 
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.86912


> rm(model)
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value+risk_factor",
+                                                    "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                    "costBin+A1+B1+C1+D1+E1+F1+G1+A2+B2+C2+D2+E2+F2+G2",
+                                                    "costDir+costDir1"),collapse="+"))),
+ #                "costDir+costDir1"),collapse="+"))),
+ #            cv.folds = 3,
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot 
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0502
     2        1.3590             nan     0.0100    0.0482
     3        1.3328             nan     0.0100    0.0462
     4        1.3076             nan     0.0100    0.0444
     5        1.2834             nan     0.0100    0.0427
     6        1.2601             nan     0.0100    0.0410
     7        1.2377             nan     0.0100    0.0395
     8        1.2162             nan     0.0100    0.0381
     9        1.1953             nan     0.0100    0.0367
    10        1.1752             nan     0.0100    0.0354
    20        1.0076             nan     0.0100    0.0254
    40        0.7924             nan     0.0100    0.0141
    60        0.6678             nan     0.0100    0.0083
    80        0.5933             nan     0.0100    0.0049
   100        0.5478             nan     0.0100    0.0031
   120        0.5192             nan     0.0100    0.0019
   140        0.5011             nan     0.0100    0.0011
   160        0.4891             nan     0.0100    0.0007
   180        0.4808             nan     0.0100    0.0005
   200        0.4749             nan     0.0100    0.0003
   220        0.4704             nan     0.0100    0.0003
   240        0.4669             nan     0.0100    0.0002
   260        0.4641             nan     0.0100    0.0001
   280        0.4619             nan     0.0100    0.0001
   300        0.4600             nan     0.0100    0.0001
   320        0.4583             nan     0.0100    0.0001
   340        0.4568             nan     0.0100    0.0001
   360        0.4556             nan     0.0100    0.0001
   380        0.4544             nan     0.0100    0.0000
   400        0.4534             nan     0.0100    0.0000
   420        0.4524             nan     0.0100    0.0000
   440        0.4515             nan     0.0100    0.0000
   460        0.4507             nan     0.0100    0.0000
   480        0.4499             nan     0.0100    0.0000
   500        0.4492             nan     0.0100    0.0000

> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8687764



> rm(model)
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value+risk_factor",
+                                                    "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                    "costBin+A1+B1+C1+D1+E1+F1+G1+A2+B2+C2+D2+E2+F2+G2"),collapse="+"))),
+ #                "costDir+costDir1"),collapse="+"))),
+ #            cv.folds = 3,
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot 
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0503
     2        1.3589             nan     0.0100    0.0481
     3        1.3328             nan     0.0100    0.0462
     4        1.3077             nan     0.0100    0.0444
     5        1.2835             nan     0.0100    0.0426
     6        1.2603             nan     0.0100    0.0410
     7        1.2380             nan     0.0100    0.0395
     8        1.2165             nan     0.0100    0.0381
     9        1.1957             nan     0.0100    0.0367
    10        1.1756             nan     0.0100    0.0355
    20        1.0080             nan     0.0100    0.0254
    40        0.7925             nan     0.0100    0.0142
    60        0.6678             nan     0.0100    0.0083
    80        0.5933             nan     0.0100    0.0050
   100        0.5479             nan     0.0100    0.0030
   120        0.5193             nan     0.0100    0.0019
   140        0.5012             nan     0.0100    0.0011
   160        0.4893             nan     0.0100    0.0007
   180        0.4811             nan     0.0100    0.0005
   200        0.4753             nan     0.0100    0.0003
   220        0.4708             nan     0.0100    0.0003
   240        0.4674             nan     0.0100    0.0002
   260        0.4647             nan     0.0100    0.0002
   280        0.4624             nan     0.0100    0.0001
   300        0.4606             nan     0.0100    0.0001
   320        0.4589             nan     0.0100    0.0001
   340        0.4575             nan     0.0100    0.0000
   360        0.4563             nan     0.0100    0.0000
   380        0.4552             nan     0.0100    0.0000
   400        0.4542             nan     0.0100    0.0001
   420        0.4533             nan     0.0100    0.0000
   440        0.4524             nan     0.0100    0.0000
   460        0.4516             nan     0.0100   -0.0000
   480        0.4509             nan     0.0100    0.0000
   500        0.4501             nan     0.0100   -0.0000

> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8690169


> rm(model)
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value+risk_factor",
+                                                    "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                    "costBin+A1+B1+C1+D1+E1+F1+G1+A2+B2+C2+D2+E2+F2+G2"),collapse="+"))),
+ #                "costDir+costDir1"),collapse="+"))),
+ #            cv.folds = 3,
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot 
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0503
     2        1.3589             nan     0.0100    0.0481
     3        1.3328             nan     0.0100    0.0462
     4        1.3077             nan     0.0100    0.0444
     5        1.2835             nan     0.0100    0.0426
     6        1.2603             nan     0.0100    0.0410
     7        1.2380             nan     0.0100    0.0395
     8        1.2165             nan     0.0100    0.0381
     9        1.1957             nan     0.0100    0.0367
    10        1.1756             nan     0.0100    0.0355
    20        1.0080             nan     0.0100    0.0254
    40        0.7925             nan     0.0100    0.0142
    60        0.6678             nan     0.0100    0.0083
    80        0.5933             nan     0.0100    0.0050
   100        0.5479             nan     0.0100    0.0030
   120        0.5193             nan     0.0100    0.0019
   140        0.5012             nan     0.0100    0.0011
   160        0.4893             nan     0.0100    0.0007
   180        0.4811             nan     0.0100    0.0005
   200        0.4753             nan     0.0100    0.0003
   220        0.4708             nan     0.0100    0.0003
   240        0.4674             nan     0.0100    0.0002
   260        0.4647             nan     0.0100    0.0002
   280        0.4624             nan     0.0100    0.0001
   300        0.4606             nan     0.0100    0.0001
   320        0.4589             nan     0.0100    0.0001
   340        0.4575             nan     0.0100    0.0000
   360        0.4563             nan     0.0100    0.0000
   380        0.4552             nan     0.0100    0.0000
   400        0.4542             nan     0.0100    0.0001
   420        0.4533             nan     0.0100    0.0000
   440        0.4524             nan     0.0100    0.0000
   460        0.4516             nan     0.0100   -0.0000
   480        0.4509             nan     0.0100    0.0000
   500        0.4501             nan     0.0100   -0.0000

> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8690169
> rm(model)
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value+risk_factor+duration_prev_disc",
+                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "costBin+G1+G2",
+                                                     "costDir+costDir1"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0502
     2        1.3589             nan     0.0100    0.0482
     3        1.3328             nan     0.0100    0.0462
     4        1.3077             nan     0.0100    0.0444
     5        1.2836             nan     0.0100    0.0427
     6        1.2603             nan     0.0100    0.0410
     7        1.2379             nan     0.0100    0.0395
     8        1.2163             nan     0.0100    0.0381
     9        1.1956             nan     0.0100    0.0366
    10        1.1755             nan     0.0100    0.0354
    20        1.0080             nan     0.0100    0.0254
    40        0.7924             nan     0.0100    0.0142
    60        0.6678             nan     0.0100    0.0083
    80        0.5933             nan     0.0100    0.0049
   100        0.5481             nan     0.0100    0.0031
   120        0.5196             nan     0.0100    0.0019
   140        0.5015             nan     0.0100    0.0011
   160        0.4895             nan     0.0100    0.0007
   180        0.4814             nan     0.0100    0.0005
   200        0.4754             nan     0.0100    0.0004
   220        0.4711             nan     0.0100    0.0003
   240        0.4677             nan     0.0100    0.0002
   260        0.4651             nan     0.0100    0.0001
   280        0.4629             nan     0.0100    0.0001
   300        0.4611             nan     0.0100    0.0001
   320        0.4595             nan     0.0100    0.0001
   340        0.4582             nan     0.0100    0.0001
   360        0.4570             nan     0.0100    0.0000
   380        0.4558             nan     0.0100    0.0000
   400        0.4548             nan     0.0100    0.0000
   420        0.4539             nan     0.0100    0.0000
   440        0.4531             nan     0.0100    0.0000
   460        0.4523             nan     0.0100    0.0000
   480        0.4516             nan     0.0100   -0.0000
   500        0.4509             nan     0.0100   -0.0000

> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.869498
> table(testRecs$G1 == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8661306

> rm(model)
Warning message:
In rm(model) : object 'model' not found
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+G1",
+                                                     "G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot 
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0502
     2        1.3591             nan     0.0100    0.0481
     3        1.3329             nan     0.0100    0.0463
     4        1.3077             nan     0.0100    0.0443
     5        1.2835             nan     0.0100    0.0427
     6        1.2603             nan     0.0100    0.0410
     7        1.2380             nan     0.0100    0.0395
     8        1.2164             nan     0.0100    0.0381
     9        1.1955             nan     0.0100    0.0367
    10        1.1755             nan     0.0100    0.0354
    20        1.0078             nan     0.0100    0.0254
    40        0.7925             nan     0.0100    0.0142
    60        0.6679             nan     0.0100    0.0082
    80        0.5935             nan     0.0100    0.0049
   100        0.5483             nan     0.0100    0.0030
   120        0.5203             nan     0.0100    0.0019
   140        0.5027             nan     0.0100    0.0011
   160        0.4916             nan     0.0100    0.0006
   180        0.4844             nan     0.0100    0.0005
   200        0.4795             nan     0.0100    0.0003
   220        0.4761             nan     0.0100    0.0002
   240        0.4737             nan     0.0100    0.0001
   260        0.4721             nan     0.0100    0.0001
   280        0.4708             nan     0.0100    0.0000
   300        0.4697             nan     0.0100    0.0000
   320        0.4689             nan     0.0100    0.0000
   340        0.4683             nan     0.0100   -0.0000
   360        0.4677             nan     0.0100   -0.0000
   380        0.4672             nan     0.0100   -0.0000
   400        0.4668             nan     0.0100   -0.0000
   420        0.4664             nan     0.0100   -0.0000
   440        0.4661             nan     0.0100   -0.0000
   460        0.4658             nan     0.0100   -0.0000
   480        0.4656             nan     0.0100   -0.0000
   500        0.4653             nan     0.0100   -0.0000

> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8681923
> table(testRecs$G1 == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8661306
>

> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value+risk_factor+duration_prev_disc",
+ #                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "costBin+G1+G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot 
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0502
     2        1.3590             nan     0.0100    0.0482
     3        1.3328             nan     0.0100    0.0462
     4        1.3077             nan     0.0100    0.0443
     5        1.2836             nan     0.0100    0.0427
     6        1.2603             nan     0.0100    0.0409
     7        1.2378             nan     0.0100    0.0395
     8        1.2162             nan     0.0100    0.0381
     9        1.1954             nan     0.0100    0.0367
    10        1.1753             nan     0.0100    0.0353
    20        1.0077             nan     0.0100    0.0253
    40        0.7924             nan     0.0100    0.0142
    60        0.6678             nan     0.0100    0.0083
    80        0.5935             nan     0.0100    0.0049
   100        0.5482             nan     0.0100    0.0030
   120        0.5201             nan     0.0100    0.0018
   140        0.5024             nan     0.0100    0.0011
   160        0.4910             nan     0.0100    0.0007
   180        0.4832             nan     0.0100    0.0005
   200        0.4779             nan     0.0100    0.0003
   220        0.4739             nan     0.0100    0.0002
   240        0.4709             nan     0.0100    0.0002
   260        0.4687             nan     0.0100    0.0001
   280        0.4668             nan     0.0100    0.0001
   300        0.4653             nan     0.0100    0.0001
   320        0.4640             nan     0.0100    0.0000
   340        0.4629             nan     0.0100    0.0000
   360        0.4619             nan     0.0100    0.0000
   380        0.4611             nan     0.0100    0.0000
   400        0.4603             nan     0.0100    0.0000
   420        0.4595             nan     0.0100    0.0000
   440        0.4589             nan     0.0100    0.0000
   460        0.4582             nan     0.0100   -0.0000
   480        0.4577             nan     0.0100    0.0000
   500        0.4571             nan     0.0100   -0.0000

> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8691887





> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner",
+ #                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "G1+G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=TRUE) # don't print out progressplot 
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        1.3863             nan     0.0100    0.0503
     2        1.3590             nan     0.0100    0.0482
     3        1.3328             nan     0.0100    0.0461
     4        1.3077             nan     0.0100    0.0443
     5        1.2835             nan     0.0100    0.0426
     6        1.2602             nan     0.0100    0.0411
     7        1.2378             nan     0.0100    0.0395
     8        1.2162             nan     0.0100    0.0381
     9        1.1954             nan     0.0100    0.0367
    10        1.1753             nan     0.0100    0.0354
    20        1.0077             nan     0.0100    0.0254
    40        0.7924             nan     0.0100    0.0142
    60        0.6678             nan     0.0100    0.0083
    80        0.5934             nan     0.0100    0.0049
   100        0.5484             nan     0.0100    0.0030
   120        0.5204             nan     0.0100    0.0018
   140        0.5029             nan     0.0100    0.0011
   160        0.4918             nan     0.0100    0.0007
   180        0.4844             nan     0.0100    0.0004
   200        0.4795             nan     0.0100    0.0002
   220        0.4760             nan     0.0100    0.0002
   240        0.4735             nan     0.0100    0.0001
   260        0.4716             nan     0.0100    0.0001
   280        0.4701             nan     0.0100    0.0000
   300        0.4690             nan     0.0100    0.0000
   320        0.4681             nan     0.0100    0.0000
   340        0.4674             nan     0.0100   -0.0000
   360        0.4668             nan     0.0100    0.0000
   380        0.4662             nan     0.0100   -0.0000
   400        0.4657             nan     0.0100   -0.0000
   420        0.4653             nan     0.0100   -0.0000
   440        0.4649             nan     0.0100   -0.0000
   460        0.4646             nan     0.0100   -0.0000
   480        0.4643             nan     0.0100   -0.0000
   500        0.4640             nan     0.0100   -0.0000

> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8683984



> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+car_value",
+ #                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "G1+G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=FALSE) # don't print out progressplot 
> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8678487




> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+homeowner+risk_factor",
+ #                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "G1+G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=FALSE) # don't print out progressplot 
> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.86912

> rm(model)
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("state+risk_factor",
+ #                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "G1+G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=FALSE) # don't print out progressplot 
> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8691544


> rm(model)
> model <- gbm(as.formula(paste0(outVar,"~", paste0(c("risk_factor",
+ #                                                     "married_couple+carAgeBin+C_previous+shopping_pt",
+                                                     "G1+G2"),collapse="+"))),
+              data= trainRecs, # dataset
+              distribution="multinomial", # see the help for other choices
+              n.trees=numTrees, # number of trees)
+              shrinkage=0.01, # shrinkage or learning rate,
+              # 0.001 to 0.1 usually work
+              interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
+              bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
+              n.minobsinnode = 10, # minimum total weight needed in each node
+              verbose=FALSE) # don't print out progressplot 
> 
> prob<-as.data.frame(predict(model,newdata=testRecs, 
+                             type="response", 
+                             n.trees=numTrees)) 
> colnames(prob) <- levels(testRecs[,outVar])
> pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
> colnames(pred_class) <- outVar
> table(pred_class$G == testRecs$G)[[2]]/nrow(testRecs)
[1] 0.8665773




A-G only
Your Best Entry
Your submission scored 0.48073, which is not an improvement of your best score. Keep trying!

G only
Your Best Entry
Your submission scored 0.54098, which is not an improvement of your best score. Keep trying!

F only
Your Best Entry
Your submission scored 0.46937, which is not an improvement of your best score. Keep trying!


#Train/Test Recs



numTrees <- 500
# trainData <- trainAug
# testData <- testAug
trainData <- trainRecs
testData <- testRecs

outVars <- c("A", "B", "C", "D", "E", "F", "G")
predOut <- data.frame(testData$customer_ID)
model <- NULL
for (outVar in outVars) {
  
  print(paste0("Training for: ", outVar))
  
  rm(model)
  model <- gbm(as.formula(paste0(outVar,"~", 
               paste0(c("state+homeowner+car_value+risk_factor+duration_prev_disc",
                                    "married_couple+carAgeBin+C_previous+shopping_pt",
                                    "costBin+A1+B1+C1+D1+E1+F1+G1+A2+B2+C2+D2+E2+F2+G2",
                                    "costDir+costDir1"),collapse="+"))),
               data= trainData, # dataset
               distribution="multinomial", # see the help for other choices
               n.trees=numTrees, # number of trees)
               shrinkage=0.01, # shrinkage or learning rate,
               # 0.001 to 0.1 usually work
               interaction.depth=5, # 1: additive model, 2: two-way interactions, etc.
               bag.fraction = 0.5, # subsampling fraction, 0.5 is probably best
               n.minobsinnode = 10, # minimum total weight needed in each node
               verbose=FALSE) # don't print out progressplot 
  
  prob<-as.data.frame(predict(model,newdata=testData, 
                              type="response", 
                              n.trees=numTrees)) 
  
  colnames(prob) <- levels(testData[,outVar])
  pred_class <- data.frame(apply(prob,1,function(x) return(names(which.max(x))[1])))
  colnames(pred_class) <- outVar
  
  predOut <- cbind(predOut,pred_class[,outVar])
  
}
colnames(predOut) <- c("customer_ID", outVars)

> table(predOut$G == testRecs$G)

FALSE  TRUE 
 3929 25174 
> table(testRecs$G1 == testRecs$G)

FALSE  TRUE 
 4021 25082

 > table(predOut$F == testRecs$F)

FALSE  TRUE 
 2105 26998 
> table(testRecs$F1 == testRecs$F)

FALSE  TRUE 
 2120 26983 
> table(predOut$E == testRecs$E)

FALSE  TRUE 
 1757 27346 
> table(testRecs$E1 == testRecs$E)

FALSE  TRUE 
 1767 27336 
> table(predOut$D == testRecs$D)

FALSE  TRUE 
 1428 27675 
> table(testRecs$D1 == testRecs$D)

FALSE  TRUE 
 1441 27662 
> table(predOut$C == testRecs$C)

FALSE  TRUE 
 1922 27181 
> table(testRecs$C1 == testRecs$C)

FALSE  TRUE 
 1926 27177 
> table(testRecs$B1 == testRecs$B)

FALSE  TRUE 
 1943 27160 
> table(predOut$B == testRecs$B)

FALSE  TRUE 
 1943 27160 
> table(predOut$A == testRecs$A)

FALSE  TRUE 
 2020 27083 
> table(testRecs$A1 == testRecs$A)

FALSE  TRUE 
 2076 27027 
 
 
 > for (x in LETTERS[1:6]) { cat("Evaluating G combo with: ", x); print(table(predOut[,x] == testRecs[,x] & predOut$G == testRecs$G))}
Evaluating G combo with:  A
FALSE  TRUE 
 5404 23699 
Evaluating G combo with:  B
FALSE  TRUE 
 5443 23660 
Evaluating G combo with:  C
FALSE  TRUE 
 5432 23671 
Evaluating G combo with:  D
FALSE  TRUE 
 5039 24064 
Evaluating G combo with:  E
FALSE  TRUE 
 5318 23785 
Evaluating G combo with:  F
FALSE  TRUE 
 5487 23616 
> 


